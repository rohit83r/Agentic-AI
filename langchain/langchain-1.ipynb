{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de48628b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Langchain 1\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "072ac5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGSMITH_TRACING=\"true\"\n",
    "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGSMITH_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "LANGSMITH_PROJECT=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d2f5b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith tracking and tracing\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cec32916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001D98CF919A0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001D98CF91C70> root_client=<openai.OpenAI object at 0x000001D98BCA87D0> root_async_client=<openai.AsyncOpenAI object at 0x000001D98CF91A90> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm= ChatOpenAI(\n",
    "    model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "f756bae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tracing using LangChainTracerV1 is no longer supported. Please set the LANGCHAIN_TRACING_V2 environment variable to enable tracing instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# have paid version to use gpt-4o\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Example usage of the LLM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m result = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the capital of France?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Agentic\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:372\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    362\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    367\u001b[39m     **kwargs: Any,\n\u001b[32m    368\u001b[39m ) -> BaseMessage:\n\u001b[32m    369\u001b[39m     config = ensure_config(config)\n\u001b[32m    370\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    371\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m372\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    382\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Agentic\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:957\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    948\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    949\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    950\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    954\u001b[39m     **kwargs: Any,\n\u001b[32m    955\u001b[39m ) -> LLMResult:\n\u001b[32m    956\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m957\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Agentic\\venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:748\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    742\u001b[39m options = {\u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop, **ls_structured_output_format_dict}\n\u001b[32m    743\u001b[39m inheritable_metadata = {\n\u001b[32m    744\u001b[39m     **(metadata \u001b[38;5;129;01mor\u001b[39;00m {}),\n\u001b[32m    745\u001b[39m     **\u001b[38;5;28mself\u001b[39m._get_ls_params(stop=stop, **kwargs),\n\u001b[32m    746\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m callback_manager = \u001b[43mCallbackManager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfigure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m    \u001b[49m\u001b[43minheritable_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    757\u001b[39m messages_to_trace = [\n\u001b[32m    758\u001b[39m     _format_for_tracing(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    759\u001b[39m ]\n\u001b[32m    760\u001b[39m run_managers = callback_manager.on_chat_model_start(\n\u001b[32m    761\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    762\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    767\u001b[39m     batch_size=\u001b[38;5;28mlen\u001b[39m(messages),\n\u001b[32m    768\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Agentic\\venv\\Lib\\site-packages\\langchain_core\\callbacks\\manager.py:1625\u001b[39m, in \u001b[36mCallbackManager.configure\u001b[39m\u001b[34m(cls, inheritable_callbacks, local_callbacks, verbose, inheritable_tags, local_tags, inheritable_metadata, local_metadata)\u001b[39m\n\u001b[32m   1594\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1595\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconfigure\u001b[39m(\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1603\u001b[39m     local_metadata: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1604\u001b[39m ) -> CallbackManager:\n\u001b[32m   1605\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Configure the callback manager.\u001b[39;00m\n\u001b[32m   1606\u001b[39m \n\u001b[32m   1607\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1623\u001b[39m \u001b[33;03m        CallbackManager: The configured callback manager.\u001b[39;00m\n\u001b[32m   1624\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_configure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1626\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1627\u001b[39m \u001b[43m        \u001b[49m\u001b[43minheritable_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43minheritable_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_tags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43minheritable_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\Agentic\\venv\\Lib\\site-packages\\langchain_core\\callbacks\\manager.py:2408\u001b[39m, in \u001b[36m_configure\u001b[39m\u001b[34m(callback_manager_cls, inheritable_callbacks, local_callbacks, inheritable_tags, local_tags, inheritable_metadata, local_metadata, verbose)\u001b[39m\n\u001b[32m   2401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m v1_tracing_enabled_ \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_v2_enabled_:\n\u001b[32m   2402\u001b[39m     \u001b[38;5;66;03m# if both are enabled, can silently ignore the v1 tracer\u001b[39;00m\n\u001b[32m   2403\u001b[39m     msg = (\n\u001b[32m   2404\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTracing using LangChainTracerV1 is no longer supported. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2405\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease set the LANGCHAIN_TRACING_V2 environment variable to enable \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtracing instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2407\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2408\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[32m   2410\u001b[39m tracer_project = _get_tracer_project()\n\u001b[32m   2411\u001b[39m debug = _get_debug()\n",
      "\u001b[31mRuntimeError\u001b[39m: Tracing using LangChainTracerV1 is no longer supported. Please set the LANGCHAIN_TRACING_V2 environment variable to enable tracing instead."
     ]
    }
   ],
   "source": [
    "# have paid version to use gpt-4o\n",
    "# Example usage of the LLM\n",
    "result = llm.invoke(\"What is the capital of France?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7607beec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x000001D98CF93980> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D98CF93E30> model_name='gemma2-9b-it' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "# free models \n",
    "from langchain_groq import ChatGroq\n",
    "model= ChatGroq(model=\"gemma2-9b-it\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1fa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is **Paris**. ðŸ‡«ðŸ‡·  \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 16, 'total_tokens': 31, 'completion_time': 0.027272727, 'prompt_time': 0.001856109, 'queue_time': 0.242995081, 'total_time': 0.029128836}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--838ef662-e9d5-4165-bf9e-a30d32d3d19e-0', usage_metadata={'input_tokens': 16, 'output_tokens': 15, 'total_tokens': 31})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\")  # Example usage of the Groq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad932bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badc7452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001D98B91EE00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D98BC447C0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chaining LLMs\n",
    "chain = prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e059433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Let\\'s break down CNNs (Convolutional Neural Networks) in computer vision!\\n\\n**What are CNNs?**\\n\\nImagine you\\'re trying to teach a computer to recognize a cat in a picture. You wouldn\\'t just show it the whole image at once â€“ you\\'d likely point out features like pointy ears, whiskers, and a fluffy tail. CNNs work in a similar way.\\n\\nThey are a specialized type of artificial neural network designed specifically for processing visual information (images and videos). \\n\\n**Key Features of CNNs:**\\n\\n* **Convolutional Layers:** These are the heart of CNNs. They use filters (also called kernels) to scan the image, detecting specific features at different scales. Think of these filters as looking for edges, corners, textures, or even simple shapes like circles or squares.\\n\\n* **Pooling Layers:** After convolution, pooling layers reduce the size of the feature maps, making the network more efficient and robust to slight variations in the image. Imagine shrinking the image while still preserving its essential features.\\n\\n* **Activation Functions:**  These introduce non-linearity into the network, allowing it to learn complex patterns and relationships. They help the CNN make more nuanced decisions about what it\\'s seeing.\\n\\n* **Fully Connected Layers:**  At the end of the convolutional and pooling layers, fully connected layers (like in traditional neural networks) process the extracted features to make a final classification or prediction (e.g., \"cat,\" \"dog,\" \"car\").\\n\\n**How CNNs Learn:**\\n\\nCNNs are trained using massive datasets of labeled images. During training, the network learns to adjust its filters and weights to minimize the difference between its predictions and the actual labels.  This process is called backpropagation.\\n\\n**Applications in Computer Vision:**\\n\\nCNNs have revolutionized computer vision, enabling:\\n\\n* **Image Classification:** Identifying objects in images (cats, dogs, cars, etc.).\\n\\n* **Object Detection:** Locating and classifying multiple objects within an image.\\n\\n* **Image Segmentation:** Dividing an image into meaningful regions (e.g., separating the sky from the ground).\\n\\n* **Facial Recognition:**  Identifying individuals from facial features.\\n\\n* **Medical Imaging:** Diagnosing diseases from X-rays, CT scans, and MRIs.\\n\\n* **Self-Driving Cars:**  Enabling vehicles to perceive and navigate their surroundings.\\n\\n**In Essence:**\\n\\nCNNs are powerful tools that allow computers to \"see\" and understand images in a way that\\'s closer to how humans do. They learn to recognize patterns and features, making them incredibly versatile for a wide range of computer vision tasks.\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 545, 'prompt_tokens': 21, 'total_tokens': 566, 'completion_time': 0.990909091, 'prompt_time': 0.002090248, 'queue_time': 0.245029043, 'total_time': 0.992999339}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--cffca78a-29c2-4c3f-a5d0-97ec92e26bac-0' usage_metadata={'input_tokens': 21, 'output_tokens': 545, 'total_tokens': 566}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"Explain CNN in computer vision.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e5b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you're trying to teach a computer to recognize cats in pictures. You could try showing it thousands of cat pictures and hoping it figures out the patterns on its own, but that's not very efficient. \n",
      "\n",
      "That's where Convolutional Neural Networks (CNNs) come in. They're like specialized brains designed specifically for understanding images.\n",
      "\n",
      "Here's a simplified explanation:\n",
      "\n",
      "**1. Convolutional Layers:** These are the heart of CNNs. They use filters (also called kernels) that scan the image, looking for specific patterns. Think of these filters like magnifying glasses that highlight edges, corners, or textures.\n",
      "\n",
      "**2. Pooling Layers:**  After convolution, these layers summarize the information from the previous layer. They essentially shrink the image while retaining the most important features. Imagine a pool party where you only remember the general vibe and not every individual person.\n",
      "\n",
      "**3. Fully Connected Layers:** These layers are similar to those in traditional neural networks. They take the processed information from the convolutional and pooling layers and use it to make a prediction, like \"cat\" or \"not cat.\"\n",
      "\n",
      "**Why are CNNs so good at computer vision?**\n",
      "\n",
      "* **Feature Extraction:** They learn hierarchical features. Early layers detect simple patterns (edges, lines), while later layers combine these into more complex features (shapes, objects). This mimics how humans see.\n",
      "* **Spatial Invariance:** They can recognize objects regardless of their location in the image.\n",
      "\n",
      "* **Parameter Sharing:** Filters are applied across the entire image, reducing the number of parameters needed and making them more efficient.\n",
      "\n",
      "**Examples of CNN Applications:**\n",
      "\n",
      "* **Image Classification:** Identifying objects in images (cats, dogs, cars, etc.)\n",
      "* **Object Detection:** Locating and classifying multiple objects in an image.\n",
      "* **Image Segmentation:** Dividing an image into regions based on object boundaries.\n",
      "* **Facial Recognition:** Identifying individuals from facial features.\n",
      "\n",
      "CNNs have revolutionized computer vision, enabling machines to \"see\" and understand the world around them with increasing accuracy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Output Parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "# Example usage of the output parser\n",
    "chain = prompt | model | output_parser\n",
    "response = chain.invoke({\"question\": \"Explain CNN in computer vision.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2ea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### JSON Output Parser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "json_output_parser = JsonOutputParser()\n",
    "json_output_parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337715ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "json_output_parser = JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the query \\n {format_instructions}\\n {question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": json_output_parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057140f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template='Answer the query \\n {format_instructions}\\n {question}')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f2d94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Transformers', 'description': 'A type of deep learning architecture specifically designed for natural language processing (NLP) tasks.', 'key_features': ['**Attention Mechanism:** Transformers leverage the attention mechanism to weigh the importance of different words in a sentence, allowing them to capture long-range dependencies.', '**Encoder-Decoder Structure:** Most transformers consist of an encoder that processes the input text and a decoder that generates the output.', '**Parallel Processing:** Unlike recurrent neural networks, transformers can process entire sequences of words in parallel, significantly speeding up training.', '**Self-Attention:**  Transformers use self-attention to allow each word in a sentence to attend to all other words, enabling a deeper understanding of context.', '**Positional Encodings:** Since transformers lack inherent sequential processing, they use positional encodings to provide information about the order of words.', '**Multi-Head Attention:** Transformers often employ multiple attention heads to capture different aspects of relationships between words.'], 'popular_architectures': ['BERT', 'GPT-3', 'T5', 'RoBERTa', 'XLNet'], 'applications': ['**Text Classification:** Sentiment analysis, topic modeling, spam detection.', '**Machine Translation:** Translating text from one language to another.', '**Question Answering:** Finding answers to questions in a given text.', '**Text Summarization:** Generating concise summaries of longer texts.', '**Dialogue Systems:** Creating chatbots and conversational agents.']}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt | model | json_output_parser\n",
    "response =chain.invoke({\"question\": \"tell me about transformmers in NLP.\"});print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c435eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e39dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc3ed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```xml\\n<response>\\n  <name>Langsmith</name>\\n  <description>Langsmith is an open-source platform for developing and deploying large language models (LLMs). It provides a modular and extensible framework for building, training, and fine-tuning LLMs, as well as tools for deploying and managing them in production.</description>\\n  <features>\\n    <feature>Modular and extensible design</feature>\\n    <feature>Support for multiple LLM architectures</feature>\\n    <feature>Tools for data preprocessing and augmentation</feature>\\n    <feature>Fine-tuning and evaluation tools</feature>\\n    <feature>Deployment and management tools</feature>\\n  </features>\\n</response>\\n```\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 195, 'total_tokens': 353, 'completion_time': 0.287272727, 'prompt_time': 0.008892394, 'queue_time': 0.24605681399999998, 'total_time': 0.296165121}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--56049495-17d9-48fd-959b-2e501ff2573d-0' usage_metadata={'input_tokens': 195, 'output_tokens': 158, 'total_tokens': 353}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94612f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is a framework for developing applications powered by large language models (LLMs). It provides tools and components for tasks such as:\\n\\n- **Chain creation:** Combining multiple LLMs and other tools into workflows.\\n- **Memory management:** Allowing LLMs to remember past interactions in a conversation.\\n- **Prompt engineering:** Crafting effective prompts to elicit desired responses from LLMs.\\n- **Data integration:** Connecting LLMs to external data sources.\\n\\nLangChain aims to simplify the development of LLM-based applications by providing a structured and modular approach.</answer></response>\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 125, 'prompt_tokens': 39, 'total_tokens': 164, 'completion_time': 0.227272727, 'prompt_time': 0.002286529, 'queue_time': 0.243048301, 'total_time': 0.229559256}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--d4e14a04-7f06-4374-b7ee-11b6103a16d1-0' usage_metadata={'input_tokens': 39, 'output_tokens': 125, 'total_tokens': 164}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model= ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb46b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why don't scientists trust atoms? Because they make up everything!\"}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f51b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a shortened filmography for Tom Hanks, with movies enclosed in <movie></movie> tags:\n",
      "\n",
      "* <movie>Splash</movie>\n",
      "* <movie>Big</movie>\n",
      "* <movie>Philadelphia</movie>\n",
      "* <movie>Forrest Gump</movie>\n",
      "* <movie>Saving Private Ryan</movie>\n",
      "* <movie>Toy Story</movie>\n",
      "* <movie>Cast Away</movie>\n",
      "* <movie>The Da Vinci Code</movie>\n",
      "* <movie>Apollo 13</movie>\n",
      "* <movie>Cloud Atlas</movie>\n",
      "\n",
      "\n",
      "This list highlights some of his most iconic and acclaimed roles. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa69ce4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "model= ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc1950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
