{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de48628b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Langchain 1\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2f5b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Langsmith tracking and tracing\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"LANGCHAIN_PROJECT\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cec32916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000016D4A499A90> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000016D4A49A490> root_client=<openai.OpenAI object at 0x0000016D4A499F90> root_async_client=<openai.AsyncOpenAI object at 0x0000016D4A49A0D0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm= ChatOpenAI(\n",
    "    model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f756bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have paid version to use gpt-4o\n",
    "# Example usage of the LLM\n",
    "# result = llm.invoke(\"What is the capital of France?\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7607beec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<groq.resources.chat.completions.Completions object at 0x0000016D51E79A90> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000016D51E7A7B0> model_name='gemma2-9b-it' model_kwargs={} groq_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "# free models \n",
    "from langchain_groq import ChatGroq\n",
    "model= ChatGroq(model=\"gemma2-9b-it\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edc1fa64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is **Paris**. \\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 16, 'total_tokens': 28, 'completion_time': 0.021818182, 'prompt_time': 0.001828819, 'queue_time': 0.24300673, 'total_time': 0.023647001}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run--d9a8d5b5-2b9b-4bd9-9888-a28499f1a2fa-0', usage_metadata={'input_tokens': 16, 'output_tokens': 12, 'total_tokens': 28})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"What is the capital of France?\")  # Example usage of the Groq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ad932bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Prompt Engineering\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{question}\"),\n",
    "])\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "badc7452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x0000016D51E79A90>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x0000016D51E7A7B0>, model_name='gemma2-9b-it', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chaining LLMs\n",
    "chain = prompt | model\n",
    "chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e059433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='## CNNs for Computer Vision: Seeing the World Through Artificial Eyes\\n\\nImagine you\\'re teaching a computer to recognize a cat in a picture. You wouldn\\'t just show it the whole image at once, right? You\\'d point out features like pointy ears, whiskers, and a fluffy tail, helping it understand what makes a cat, a cat.\\n\\nThat\\'s essentially what Convolutional Neural Networks (CNNs) do! They\\'re a special type of artificial neural network designed specifically for processing image data.\\n\\n**Here\\'s a breakdown:**\\n\\n1. **Convolutional Layers:** Think of these as the \"feature detectors.\" They scan the image with small filters (also called kernels) that highlight specific features like edges, corners, or textures. Each filter learns to recognize a particular pattern.\\n\\n2. **Pooling Layers:** These layers downsample the image, reducing its size while retaining important information. This helps the network become more robust to small changes in the image and makes processing faster.\\n\\n3. **Fully Connected Layers:** These layers take the processed features from the convolutional and pooling layers and combine them to make a final decision. They\\'re similar to the neurons in our brain that connect different parts of our visual system.\\n\\n**How it all works together:**\\n\\n- The image is fed into the CNN, where the convolutional layers detect various features at different scales.\\n- Pooling layers summarize these features, creating a more compact representation.\\n- The fully connected layers analyze these summaries and classify the image, for example, as \"cat\" or \"not cat.\"\\n\\n**Why are CNNs so good at computer vision?**\\n\\n- **Hierarchy of Features:** CNNs learn features at different levels of abstraction, from simple edges to complex objects. This allows them to understand images in a more meaningful way.\\n- **Parameter Sharing:** Filters are used across the entire image, which reduces the number of parameters the network needs to learn, making training more efficient.\\n- **Translation Invariance:** CNNs are relatively insensitive to the position of objects in the image, meaning they can recognize cats regardless of where they are in the picture.\\n\\n**Applications of CNNs:**\\n\\nCNNs have revolutionized computer vision, powering applications like:\\n\\n- **Image Classification:** Identifying objects in images (cats, dogs, cars, etc.)\\n- **Object Detection:** Locating and classifying multiple objects in an image.\\n- **Image Segmentation:** Dividing an image into different regions based on object boundaries.\\n- **Facial Recognition:** Identifying individuals based on their facial features.\\n\\n\\nCNNs are constantly evolving, with new architectures and training techniques being developed all the time. They have become the go-to tool for tackling a wide range of computer vision tasks, bringing us closer to machines that can \"see\" and understand the world like humans.\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 580, 'prompt_tokens': 21, 'total_tokens': 601, 'completion_time': 1.054545455, 'prompt_time': 0.002059449, 'queue_time': 0.24451269, 'total_time': 1.056604904}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--baef26fb-469b-4b50-be34-56c29503b43e-0' usage_metadata={'input_tokens': 21, 'output_tokens': 580, 'total_tokens': 601}\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"question\": \"Explain CNN in computer vision.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "994e5b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you're trying to teach a computer to recognize a cat in a picture. You could try to describe all the features of a cat – pointy ears, whiskers, fluffy tail – but that's hard to do precisely and computers aren't great at understanding complex descriptions.\n",
      "\n",
      "That's where Convolutional Neural Networks (CNNs) come in! They're like specialized artificial brains designed specifically for understanding images. \n",
      "\n",
      "Here's how they work:\n",
      "\n",
      "**1. Convolutional Layers:** These layers are the heart of a CNN. They use small filters (think of them as magnifying glasses) to scan the image. Each filter looks for specific patterns, like edges, corners, or textures.\n",
      "\n",
      "**2. Pooling Layers:** After the convolutional layers, pooling layers shrink the image size while retaining the most important information. This helps make the network more efficient and robust to small changes in the image.\n",
      "\n",
      "**3. Fully Connected Layers:** Finally, the processed information from the convolutional and pooling layers is flattened and fed into fully connected layers, similar to those in a traditional neural network. These layers learn to combine the features detected earlier to make a decision, like \"This is a cat!\"\n",
      "\n",
      "**Here's an analogy:**\n",
      "\n",
      "Think of a team of detectives investigating a crime scene.\n",
      "\n",
      "* **Convolutional layers:** Each detective uses a special magnifying glass to look for specific clues, like fingerprints or footprints.\n",
      "* **Pooling layers:**  The detectives share their findings and focus on the most important clues.\n",
      "* **Fully connected layers:** The lead detective analyzes all the collected evidence and decides who the culprit is.\n",
      "\n",
      "**Why are CNNs so good at computer vision?**\n",
      "\n",
      "* **They learn hierarchical features:** CNNs can learn increasingly complex features as they go deeper. The early layers detect simple features like edges, while later layers combine these features to recognize objects.\n",
      "* **They are translation invariant:**  CNNs can recognize objects regardless of their position in the image.\n",
      "* **They are efficient:** CNNs can process images efficiently, making them suitable for real-time applications.\n",
      "\n",
      "\n",
      "CNNs have revolutionized computer vision, powering applications like image recognition, object detection, facial recognition, and self-driving cars.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Output Parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()\n",
    "# Example usage of the output parser\n",
    "chain = prompt | model | output_parser\n",
    "response = chain.invoke({\"question\": \"Explain CNN in computer vision.\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7f2ea41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### JSON Output Parser\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "json_output_parser = JsonOutputParser()\n",
    "json_output_parser.get_format_instructions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "337715ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "json_output_parser = JsonOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the query \\n {format_instructions}\\n {question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": json_output_parser.get_format_instructions()},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "057140f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['question'], input_types={}, partial_variables={'format_instructions': 'Return a JSON object.'}, template='Answer the query \\n {format_instructions}\\n {question}')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0f2d94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Transformers in NLP', 'description': 'Transformer models are a revolutionary architecture in natural language processing (NLP) known for their ability to handle long-range dependencies in text effectively. They have achieved state-of-the-art results in a wide range of NLP tasks.', 'key_features': ['**Attention Mechanism:** Transformers rely heavily on the attention mechanism, which allows the model to focus on relevant parts of the input sequence when processing each word.', '**Encoder-Decoder Structure:** Most transformer models have an encoder-decoder structure, where the encoder processes the input sequence and the decoder generates the output sequence.', '**Parallel Processing:** Transformers can process input sequences in parallel, significantly speeding up training and inference compared to recurrent neural networks.', '**Positional Encodings:** Since transformers lack inherent sequential information, they use positional encodings to embed the order of words in the input sequence.', '**Multi-Head Attention:** Transformers often employ multi-head attention, which allows the model to attend to different aspects of the input sequence simultaneously.'], 'popular_transformer_models': ['BERT', 'GPT-3', 'T5', 'RoBERTa', 'XLNet'], 'applications': ['**Text Classification:** Sentiment analysis, topic modeling, spam detection', '**Machine Translation:** Translating text from one language to another', '**Question Answering:** Finding answers to questions in a given context', '**Text Summarization:** Generating concise summaries of longer texts', '**Dialogue Systems:** Creating chatbots and conversational agents']}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt | model | json_output_parser\n",
    "response =chain.invoke({\"question\": \"tell me about transformmers in NLP.\"});print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c435eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "output_parser=XMLOutputParser()\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer.<response><answer>Your answer here</answer></response>.Provide me answer based on the question\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14e39dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['query'], input_types={}, partial_variables={'format_instruction': 'The output should be formatted as a XML file.\\n1. Output should conform to the tags below.\\n2. If tags are not given, make them on your own.\\n3. Remember to always open and close all the tags.\\n\\nAs an example, for the tags [\"foo\", \"bar\", \"baz\"]:\\n1. String \"<foo>\\n   <bar>\\n      <baz></baz>\\n   </bar>\\n</foo>\" is a well-formatted instance of the schema.\\n2. String \"<foo>\\n   <bar>\\n   </foo>\" is a badly-formatted instance.\\n3. String \"<foo>\\n   <tag>\\n   </tag>\\n</foo>\" is a badly-formatted instance.\\n\\nHere are the output tags:\\n```\\nNone\\n```'}, template='Answer the user query \\n {format_instruction}\\n {query}\\n ')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### OutputParser\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser=XMLOutputParser()\n",
    "\n",
    "prompt=PromptTemplate(\n",
    "    template=\"Answer the user query \\n {format_instruction}\\n {query}\\n \",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instruction\":output_parser.get_format_instructions()},\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cc3ed11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response>\\n  <name>Langsmith</name>\\n  <description>Langsmith is an open-source platform for developing and deploying large language models (LLMs). It provides a framework for training, evaluating, and fine-tuning LLMs, as well as tools for deploying them as APIs.</description>\\n  <features>\\n    <feature>Modular and extensible architecture</feature>\\n    <feature>Support for multiple LLM architectures</feature>\\n    <feature>Comprehensive set of training and evaluation tools</feature>\\n    <feature>Easy deployment as APIs</feature>\\n  </features>\\n  <creator>The Langsmith team</creator>\\n</response> \\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 148, 'prompt_tokens': 195, 'total_tokens': 343, 'completion_time': 0.269090909, 'prompt_time': 0.008926806, 'queue_time': 0.24496708299999997, 'total_time': 0.278017715}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--cce48146-d46c-4c5b-9253-a2833d35e6c7-0' usage_metadata={'input_tokens': 195, 'output_tokens': 148, 'total_tokens': 343}\n"
     ]
    }
   ],
   "source": [
    "chain=prompt|model\n",
    "response=chain.invoke({\"query\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94612f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<response><answer>LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides tools and components for tasks like prompting, memory management, chaining together different LLMs and other AI services, and integrating with external data sources.</answer></response>\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 39, 'total_tokens': 105, 'completion_time': 0.12, 'prompt_time': 0.002265799, 'queue_time': 0.24448298999999998, 'total_time': 0.122265799}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run--9ef7525c-34cf-4648-a652-fc0a0e2e8613-0' usage_metadata={'input_tokens': 39, 'output_tokens': 66, 'total_tokens': 105}\n"
     ]
    }
   ],
   "source": [
    "##output parser\n",
    "#from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain.output_parsers.xml import XMLOutputParser\n",
    "\n",
    "# XML Output Parser\n",
    "output_parser = XMLOutputParser()\n",
    "\n",
    "# Prompt that instructs the model to return XML\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant. Respond in this XML format: <response><answer>Your answer here</answer></response>\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Build the chain\n",
    "chain = prompt | model\n",
    "\n",
    "# Run the chain\n",
    "#response = chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "raw_output =chain.invoke({\"input\": \"What is LangChain?\"})\n",
    "\n",
    "# Print result\n",
    "print(raw_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9110d703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'setup': \"Why don't scientists trust atoms?\",\n",
       " 'punchline': 'Because they make up everything!'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## With Pydantic\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "model= ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dcb46b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'joke': \"Why don't scientists trust atoms? Because they make up everything!\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Without Pydantic\n",
    "joke_query = \"Tell me a joke .\"\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2f51b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a shortened filmography for Tom Hanks, with movies enclosed in <movie></movie> tags:\n",
      "\n",
      "* <movie>Splash</movie>\n",
      "* <movie>Big</movie>\n",
      "* <movie>Philadelphia</movie>\n",
      "* <movie>Forrest Gump</movie>\n",
      "* <movie>Saving Private Ryan</movie>\n",
      "* <movie>Toy Story</movie>\n",
      "* <movie>Cast Away</movie>\n",
      "* <movie>The Da Vinci Code</movie>\n",
      "* <movie>Apollo 13</movie> \n",
      "\n",
      "\n",
      "This list highlights some of his most popular and critically acclaimed roles. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.output_parsers import XMLOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "actor_query = \"Generate the shortened filmography for Tom Hanks.\"\n",
    "\n",
    "output = model.invoke(\n",
    "    f\"\"\"{actor_query}\n",
    "Please enclose the movies in <movie></movie> tags\"\"\"\n",
    ")\n",
    "\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa69ce4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Joke(setup=\"Why don't scientists trust atoms?\", punchline='Because they make up everything!')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import YamlOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "model= ChatGroq(model=\"gemma2-9b-it\")\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Joke(BaseModel):\n",
    "    setup: str = Field(description=\"question to set up a joke\")\n",
    "    punchline: str = Field(description=\"answer to resolve the joke\")\n",
    "\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "joke_query = \"Tell me a joke.\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = YamlOutputParser(pydantic_object=Joke)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"query\": joke_query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc1950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
